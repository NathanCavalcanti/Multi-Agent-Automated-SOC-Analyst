# ğŸ›°ï¸ SOC Multi-Agent AI Assistant  
Multi-agent SOC/DFIR automation system built with LangChain, LangGraph, and **Groq Llama 3.1 models (free, ultra-fast LLM inference)**.

This project automates:
- IOC extraction  
- MITRE ATT&CK mapping  
- CVE correlation using OSINT feeds  
- DFIR investigation planning  
- Professional incident report generation  

The system uses **GroqCloud API** to run Llama 3.1 for free while maintaining high performance.

---

## ğŸš€ LLM Provider

This project uses **Groq** instead of OpenAI.

### Models used (Groq):
- `llama3-70b-8192` â†’ Reasoning + Agents  
- `llama3-8b-8192` â†’ Fast low-cost extraction  
- `mixtral-8x7b-32768` (optional) â†’ Structured DFIR output

You must export your Groq API key:

```
set GROQ_API_KEY=your_key_here
```

---

## ğŸ“¦ Installation

```
pip install langchain langgraph groq fastapi uvicorn chromadb python-dotenv
```

---

## ğŸ”§ Configuration

Create `.env`:

```
GROQ_API_KEY=your_key_here
LLM_MODEL=llama3-70b-8192
```

---

## ğŸ“ Repository Structure

(same structure as before)

---

## ğŸ¤– Agents (Groq-powered)

- Agent 1: IOC Extractor â†’ llama3-8b  
- Agent 2: MITRE Mapper â†’ llama3-70b  
- Agent 3: CVE Retriever â†’ llama3-70b  
- Agent 4: DFIR Planner â†’ mixtral-8x7b  
- Agent 5: Report Writer â†’ llama3-70b

---

## ğŸŒ API

POST `/api/process_incident` â†’ runs entire LangGraph multi-agent pipeline using Groq LLMs.

---

## âš¡ Why Groq?

- Free tier (no credits)
- Ultra-low latency (<40 ms)
- State-of-the-art Llama 3.1 models
- Stable inference for multi-agent workflows
